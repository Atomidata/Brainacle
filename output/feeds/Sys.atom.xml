<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Brainacle</title><link href="http://brainacle.com" rel="alternate"></link><link href="http://brainacle.com/feeds/Sys.atom.xml" rel="self"></link><id>http://brainacle.com</id><updated>2010-06-15T23:17:00Z</updated><entry><title>Pgbouncer Makes a Difference</title><link href="http://brainacle.com/pgbouncer-makes-a-difference.html" rel="alternate"></link><updated>2010-06-15T23:17:00Z</updated><author><name>Vasil Vangelovski</name></author><id>tag:brainacle.com,2010-06-15:/pgbouncer-makes-a-difference.html/</id><summary type="html">&lt;p&gt;Last week I was building VMware images for database and web server
appliances that would host a fairly large Django application. The
application is backed by a PostgreSQL database and I was looking for
some info on compiling/configuring pgpool on Debian (I like to compile
stuff when I can, especially when the last version of Debian is 2
years old). Googling around I came across some very interesting posts
on mailing lists and SO regarding Django, PostgeSQL and connection
pooling.&lt;/p&gt;
&lt;p&gt;Among other things, people seem to have a notion that using pooling
middleware won’t accomplish much as the web server still needs to open
a TCP connection and that is the source of a noticeable overhead for
each request. So they’ve come up with solutions to avoid opening TCP
connections as much as possible, trying to accomplish something
similar to what SQLAlchemy’s connection pool does, keeping the web
server connected to the database with multiple connections at all
times. These solutions of course range from changing the code in
django.db.backends.... to monkey-patching it.&lt;/p&gt;
&lt;p&gt;When you change Django’s code you’ve just created a fork of a growing
and evolving open source project and based your own project around
that fork you have to maintain yourself. Monkey-patching is not as
bad, but comes very close regarding maintenance problems. And is all
that really necessary?&lt;/p&gt;
&lt;div class="section" id="the-overhead-doesnt-stem-from-opening-tcp-connections"&gt;
&lt;h2&gt;The Overhead Doesn’t Stem From Opening TCP Connections&lt;/h2&gt;
&lt;p&gt;Every time you open a database connection (session) to execute some
SQL on a Postgres database the Postgres server spawns a new process
and upon closing the connection (session) from your application that
process is shut down. With the way Django handles database sessions
this is repeated for every request. Which means for every request
Postgres will have to spawn a new worker process that will last for
the duration of the database session involved in responding to that
HTTP request. The overhead involved in opening a TCP connection to a
process running on the same machine or on the same network is not much
compared to the overhead involved in spawning a new process.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="solutions-do-exist"&gt;
&lt;h2&gt;Solutions Do Exist&lt;/h2&gt;
&lt;p&gt;So if the overhead for each request comes from spawning new processes
then the obvious solution would be to keep that at a minimum level. If
you keep the connections to the database server open and reuse them
for every request then the processes spawned at the time the
connections were established would be reused as the connections are
reused.&lt;/p&gt;
&lt;p&gt;But you don’t have to keep your web server connected to the database
server to achieve this. Two more popular solutions are pgpool II and
pgbouncer. Both are designed as sort of middleware proxies that sit
between your application and your database. Pgpool is more of a
replication and load balancing solution than a connection pool. It
works as a connection pool because at each connection opened by your
application to pgpool it will have a separate process handling that
connection, but it will keep those processes alive and connected to
the Postgres server even after your application closes those
connections. So using it would have the effect of lowering the net
amount of new processes created to serve a certain number of requests
to your web application. Pgbouncer on the other hand handles all the
requests between your application and Postgres in a highly efficient
asynchronous manner by utilizing libevent and not using
multiprocessing at all, and it will keep the initially opened
connections for a longer time after your application closes them, so
making a new connection to pgbouncer will rarely result in Postges
spawning a process.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-proof"&gt;
&lt;h2&gt;The Proof&lt;/h2&gt;
&lt;p&gt;To prove that solutions like pgbouncer do make a difference I created
a simple test scenario. A very small Django project with one page
displaying 5 rows from a table in a PostgreSQL database. Both the
database server and the web server (Apache with mod-wsgi in daemon
mode) running on one small VM with 1GB of RAM and 4 CPU cores
assigned.&lt;/p&gt;
&lt;p&gt;In the first test I configured the application to connect to the
database server directly and put the page under
&lt;a class="reference external" href="http://www.joedog.org/index/siege-home"&gt;siege&lt;/a&gt; with 1, 5, 15,&lt;/p&gt;
&lt;p&gt;50, 100 and 200 concurrent requests, each session lasting for 1
minute. Then I repeated the process with the application configured to
connect to pgbouncer instead. The results show something close to a
50% increase in responsiveness:&lt;/p&gt;
&lt;img alt="https://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGktNUpCUmtsbWFoWHc5WFRjQjFXV0E&amp;amp;oid=2&amp;amp;zx=8zp184e7ixus" src="https://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGktNUpCUmtsbWFoWHc5WFRjQjFXV0E&amp;amp;oid=2&amp;amp;zx=8zp184e7ixus" /&gt;
&lt;/div&gt;
</summary><category term="django"></category><category term="pgbouncer"></category><category term="pgpool"></category><category term="postgresql"></category></entry><entry><title>Benchmark of django deployment techniques</title><link href="http://brainacle.com/benchmark-of-django-deployment-techniques.html" rel="alternate"></link><updated>2010-01-19T19:55:00Z</updated><author><name>Vasil Vangelovski</name></author><id>tag:brainacle.com,2010-01-19:/benchmark-of-django-deployment-techniques.html/</id><summary type="html">&lt;p&gt;I made a benchmark of different Django deployment techniques and configurations
mostly for my personal purposes. The results are published in the hope that it
would save others some time. The benchmark was not designed to test the speed
of Django itself, only to give relative comparison between different techniques
for running a Django application in production environments. I'm not affiliated
with any of the following open source projects: Apache,&amp;nbsp; Nginx, Cherokee,
mod_wsgi, mod_python, Cherrypy or uWSGI. Further, I'm not claiming to be an
expert in configuring any of the software mentioned here.&lt;/p&gt;
&lt;div class="section" id="what-was-measured"&gt;
&lt;h2&gt;What was measured&lt;/h2&gt;
&lt;p&gt;The Django project used for the benchmark was a simple application for
displaying rows from 3 different tables with pagination. Each page had
references to 3 static files (css, javascript and an image). Each page
involved rendering a simple template inheriting from a base template
and including another one, built-in filters were also used. The database
had more than a million records in all three tables combined. I browsed
different pages of the application over a proxy which recorded the URLs
of the browsing session. So for each request that was handled by the application
there were 3 more requests for static files. For each deployment technique I ran
4 tests at different concurrency levels for 1 minute making GET requests to the
recorded URLs. For each test run I recorded throughput (number of requests
served per second), response time (average time in which a request was served)
and longest request (the longest time a request was served in each run).
Only for the tests at highest concurrency levels I recorded memory usage.
I tried to make sure that only the necessary processes for each test were
running at a time. Automatic maintenance tasks on the system and the database
were turned off. Every test cycle was repeated at least 3 times to
recheck the results.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="hardware-and-software-details"&gt;
&lt;h2&gt;Hardware and software details&lt;/h2&gt;
&lt;p&gt;For generating the loads I used Siege and ran all the benchmarks over
gigabit ethernet from a 2.16 GHz machine with 2 GB of RAM running OS X Snow
Leopard.&lt;/p&gt;
&lt;p&gt;The system that served as a web and database server ran in a VMware appliance
on a 2.8 GHz Core2Duo PC with 8GBs of RAM. The appliance was given only 1GB
of working memory and assigned both cores of the CPU. Software details:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Ubuntu 9.10 32b&lt;/li&gt;
&lt;li&gt;Python 2.6&lt;/li&gt;
&lt;li&gt;PostgreSQL 8.4&lt;/li&gt;
&lt;li&gt;Apache 2.2.12 worker MPM&lt;/li&gt;
&lt;li&gt;Nginx 0.7.64&lt;/li&gt;
&lt;li&gt;Cherokee 0.99.39&lt;/li&gt;
&lt;li&gt;Django 1.1&lt;/li&gt;
&lt;li&gt;psycopg2 2.0.8&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="tested-configurations"&gt;
&lt;h2&gt;Tested configurations&lt;/h2&gt;
&lt;div class="section" id="apache-with-mod-wsgi"&gt;
&lt;h3&gt;Apache with mod_wsgi&lt;/h3&gt;
&lt;p&gt;This was the first configuration I tested. Apache was serving both the static
files and dynamic content via mod_wsgi which ran in daemon mode with 5 processes
and 1 thread per process.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="nginx-apache-with-mod-wsgi"&gt;
&lt;h3&gt;Nginx + Apache with mod_wsgi&lt;/h3&gt;
&lt;p&gt;My preferred configuration for running django sites. Apache with mod_wsgi
was used only for the dynamic content, requests to these urls were proxied
by nginx. Static files were served by nginx directly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="nginx-fcgi"&gt;
&lt;h3&gt;Nginx + fcgi&lt;/h3&gt;
&lt;p&gt;Here nginx is used for serving the static content while the dynamic content
was handled by FastCGI processes. I used all the defaults from the runfcgi
management command and used a TCP socket instead of a socket file because I
was bumping into issues with access to the socket file at large numbers of
concurrent requests.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cherokee-scgi"&gt;
&lt;h3&gt;Cherokee + SCGI&lt;/h3&gt;
&lt;p&gt;This was set up from the Cherokee web based wizard for deploying django
applications. Static files were served by Cherokee directly. I have to say
this is by far the&amp;nbsp; easiest method of deploying Django applications in
production environments.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cherokee-apache-with-mod-wsgi"&gt;
&lt;h3&gt;Cherokee + Apache with mod_wsgi&lt;/h3&gt;
&lt;p&gt;This is essentially the same as Nginx + Apache and mod_wsgi except here
Cherokee was used as a proxy and for serving static content. Since it's the
first time I'm using Cherokee everything was configured via the web based admin
interface and all parameters were left to default values.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="nginx-cherrypy-wsgi-server"&gt;
&lt;h3&gt;Nginx + Cherrypy WSGI server&lt;/h3&gt;
&lt;p&gt;Here I used the &lt;a class="reference external" href="http://github.com/lincolnloop/django-cpserver"&gt;django-cpserver&lt;/a&gt;
management command to run the application in the Cherrypy WSGI server. 5
Instances of the WSGI server were running behind Nginx as a load balancer.
Nginx was serving the static files.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="configurations-that-are-left-out"&gt;
&lt;h2&gt;Configurations that are left out&lt;/h2&gt;
&lt;div class="section" id="apache-with-mod-python"&gt;
&lt;h3&gt;Apache with mod_python&lt;/h3&gt;
&lt;p&gt;This was left out because I couldn't get consistent results at 250 concurrent
requests and the application would often error out at this concurrency level.
The benchmark already took a significant amount of my time and I'm not
experienced with mod_python so I decided not to proceed with locating the
problem or publishing any shaky results.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="uwsgi"&gt;
&lt;h3&gt;uWSGI&lt;/h3&gt;
&lt;p&gt;I tried to deploy the application on uWSGI with the Cherokee web based wizard.
With the default configuration (1 process) the tests ran 4 times slower at high
concurrency compared to the other configurations. Bumping up the number of
process to 5 still didn't yield comparable results. At 25 processes I got
comparable results but memory usage skyrocketed. I still suspect I was doing
something wrong here, so I didn't publish the results.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lighttpd"&gt;
&lt;h3&gt;lighttpd&lt;/h3&gt;
&lt;p&gt;Personally I avoid using lighty for a number of subjective reasons. When I have
the time I may update this post with some configurations based on lighty,
although I wouldn't expect the results to be much different than the ones
for the Nginx or Cherokee configurations.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="the-results"&gt;
&lt;h2&gt;The results&lt;/h2&gt;
&lt;a class="reference external image-reference" href="http://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGdJOFVicHNGSFJGemFRT1pGMnVQMlE&amp;amp;amp&amp;amp;oid=5&amp;amp;amp&amp;amp;v=1263914004562"&gt;&lt;img alt="http://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGdJOFVicHNGSFJGemFRT1pGMnVQMlE&amp;amp;amp&amp;amp;oid=5&amp;amp;amp&amp;amp;v=1263914004562" src="http://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGdJOFVicHNGSFJGemFRT1pGMnVQMlE&amp;amp;amp&amp;amp;oid=5&amp;amp;amp&amp;amp;v=1263914004562" /&gt;&lt;/a&gt;
&lt;a class="reference external image-reference" href="http://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGdJOFVicHNGSFJGemFRT1pGMnVQMlE&amp;amp;amp&amp;amp;oid=7&amp;amp;amp&amp;amp;v=1263914042360"&gt;&lt;img alt="http://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGdJOFVicHNGSFJGemFRT1pGMnVQMlE&amp;amp;amp&amp;amp;oid=7&amp;amp;amp&amp;amp;v=1263914042360" src="http://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGdJOFVicHNGSFJGemFRT1pGMnVQMlE&amp;amp;amp&amp;amp;oid=7&amp;amp;amp&amp;amp;v=1263914042360" /&gt;&lt;/a&gt;
&lt;a class="reference external image-reference" href="http://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGdJOFVicHNGSFJGemFRT1pGMnVQMlE&amp;amp;amp&amp;amp;oid=6&amp;amp;amp&amp;amp;v=1263914066019"&gt;&lt;img alt="http://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGdJOFVicHNGSFJGemFRT1pGMnVQMlE&amp;amp;amp&amp;amp;oid=6&amp;amp;amp&amp;amp;v=1263914066019" src="http://spreadsheets.google.com/oimg?key=0ApNjbkQcMGV4dGdJOFVicHNGSFJGemFRT1pGMnVQMlE&amp;amp;amp&amp;amp;oid=6&amp;amp;amp&amp;amp;v=1263914066019" /&gt;&lt;/a&gt;
&lt;div class="section" id="memory-usage"&gt;
&lt;h3&gt;Memory usage&lt;/h3&gt;
&lt;p&gt;I measured memory usage only at 250 concurrent requests. All the tests ran for
60 seconds, so you can work out where the load on the server happened from the
graphs.&lt;/p&gt;
&lt;div class="section" id="id1"&gt;
&lt;h4&gt;Apache with mod_wsgi&lt;/h4&gt;
&lt;a class="reference external image-reference" href="/static/uploads/djbenchmark/mod_wsgi2.png"&gt;&lt;img alt="/static/uploads/djbenchmark/mod_wsgi2-300x225.png" src="/static/uploads/djbenchmark/mod_wsgi2-300x225.png" /&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h4&gt;Nginx + Apache with mod_wsgi&lt;/h4&gt;
&lt;a class="reference external image-reference" href="/static/uploads/djbenchmark/nginxmod_wsgi3.png"&gt;&lt;img alt="/static/uploads/djbenchmark/nginxmod_wsgi3-300x225.png" src="/static/uploads/djbenchmark/nginxmod_wsgi3-300x225.png" /&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div class="section" id="id3"&gt;
&lt;h4&gt;Nginx + FCGI&lt;/h4&gt;
&lt;a class="reference external image-reference" href="/static/uploads/djbenchmark/nginxcgi1.png"&gt;&lt;img alt="/static/uploads/djbenchmark/nginxcgi1-300x225.png" src="/static/uploads/djbenchmark/nginxcgi1-300x225.png" /&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div class="section" id="nginx-cherrypy"&gt;
&lt;h4&gt;Nginx + Cherrypy&lt;/h4&gt;
&lt;a class="reference external image-reference" href="/static/uploads/djbenchmark/nginxcp1.png"&gt;&lt;img alt="/static/uploads/djbenchmark/nginxcp1-300x225.png" src="/static/uploads/djbenchmark/nginxcp1-300x225.png" /&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div class="section" id="id4"&gt;
&lt;h4&gt;Cherokee + SCGI&lt;/h4&gt;
&lt;a class="reference external image-reference" href="/static/uploads/djbenchmark/cherokeewscgi1.png"&gt;&lt;img alt="/static/uploads/djbenchmark/cherokeewscgi1-300x225.png" src="/static/uploads/djbenchmark/cherokeewscgi1-300x225.png" /&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div class="section" id="id5"&gt;
&lt;h4&gt;Cherokee + Apache with mod_wsgi&lt;/h4&gt;
&lt;a class="reference external image-reference" href="/static/uploads/djbenchmark/cherokeewsgi1.png"&gt;&lt;img alt="/static/uploads/djbenchmark/cherokeewsgi1-300x225.png" src="/static/uploads/djbenchmark/cherokeewsgi1-300x225.png" /&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary><category term="django"></category><category term="python"></category><category term="deployment"></category><category term="sys"></category><category term="benchmark"></category></entry></feed>